# Modern Data Stack Lab

A collection of end-to-end data engineering and analytics projects showcasing skills in dbt, Databricks, Azure, and modern data stack tools. Each project demonstrates solving a real-world problem with production-grade practices.

â¸»

ğŸ“‚ Projects

1. NYC Taxi Analytics (dbt + Postgres)

Tech: dbt, PostgreSQL, GitHub Actions (optional CI), VS Code Power User extension
	â€¢	Problem: NYC taxi data is large and messy (millions of rows, multiple schemas).
	â€¢	Goal: Transform raw taxi trips into clean, analytics-ready marts for insights like revenue, trip volume, and airport traffic.
	â€¢	Highlights:
	â€¢	Staging â†’ Intermediate â†’ Marts modeling with dbt
	â€¢	Incremental model for fct_trip (~3.4M rows handled efficiently)
	â€¢	Source freshness checks & dbt tests (not null, unique, constraints)
	â€¢	Exposures defined for a dashboard dependency graph
	â€¢	DAG lineage visualized via dbt Docs

ğŸ“‚ Folder: dbt-project/nyc_taxi_analytics

2. Databricks Lakehouse Project (coming soon)

Tech: Databricks, Delta Lake, PySpark
	â€¢	Focus on ingesting raw JSON/Parquet data into a bronze-silver-gold pipeline.
	â€¢	Demonstrates Delta Lake merges, deletes, and time travel.
	â€¢	Will include feature engineering with PySpark.

ğŸ“‚ Folder: databricks-lakehouse (planned)

â¸»

3. Azure Data Engineering Project (coming soon)

Tech: Azure Data Factory, Synapse, Fabric, Event Hubs
	â€¢	Event-driven ETL pipelines with ADF + Synapse.
	â€¢	Real-time ingestion with Event Hubs â†’ CosmosDB â†’ Synapse Link.
	â€¢	Cost optimization & monitoring with Application Insights.

ğŸ“‚ Folder: azure-etl (planned)

â¸»

4. AWS Data Engineering Project (coming soon)

Tech: AWS Glue, Redshift, S3, Lambda, Kinesis
	â€¢	Serverless ETL with Glue + Lambda.
	â€¢	Real-time streaming ingestion with Kinesis â†’ S3 â†’ Redshift.
	â€¢	Data Lakehouse architecture on S3 with partitioning & lifecycle policies.

ğŸ“‚ Folder: aws-etl (planned)

â¸»

5. Snowflake Data Warehouse Project (coming soon)

Tech: Snowflake, dbt, Airflow
	â€¢	Building a cloud-native data warehouse with Snowflake.
	â€¢	Orchestration and scheduling via Airflow.
	â€¢	Showcases Snowflake features: zero-copy cloning, streams & tasks, and time travel.

ğŸ“‚ Folder: snowflake-dwh (planned)

â¸»

ğŸ“Š Roadmap
	â€¢	dbt NYC Taxi Analytics project
	â€¢	Databricks Lakehouse project
	â€¢	Azure Data Engineering pipelines
	â€¢	AWS Data Engineering pipelines
	â€¢	Snowflake Data Warehouse
	â€¢	Airflow orchestration example
	â€¢	Real-time streaming (Kafka, Kinesis, Event Hubs)

â¸» 

ğŸ› ï¸ Setup Instructions (for dbt project)

Requirements
	â€¢	Python 3.11+
	â€¢	dbt-core & dbt-postgres
	â€¢	PostgreSQL running locally (or Docker)

Run Locally

# Clone repo
git clone https://github.com/williamnsambu/modern-data-stack-lab.git
cd modern-data-stack-lab/dbt-project/nyc_taxi_analytics

# Create virtual env
python3.11 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt   # or install dbt manually

# Run dbt models
export DBT_PROFILES_DIR=$(pwd)
dbt debug
dbt run
dbt test

# Generate docs
dbt docs generate
dbt docs serve

â¸»

ğŸ¤ Contributing

This is a personal lab/portfolio, but feel free to open issues or PRs with suggestions.

â¸»

ğŸ‘¨â€ğŸ’» Author

William Nsambu
Software & Data Engineer | Cloud Solutions | Modern Data Stack Enthusiast
ğŸŒ LinkedIn: www.linkedin.com/in/william-nsambu-a5467ab2 | ğŸ’» GitHub: https://github.com/williamnsambu

â¸»
